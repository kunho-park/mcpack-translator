import asyncio
import io
import json
import logging
import os
import re
import shutil
import sys
import tempfile
import time
import traceback
import uuid
import zipfile
from glob import escape as glob_escape
from glob import glob

import streamlit as st

# Windows í™˜ê²½ì—ì„œ asyncio ì´ë²¤íŠ¸ ë£¨í”„ ì •ì±… ì„¤ì •
if sys.platform.startswith("win"):
    import asyncio

    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())


from catboxpy.catbox import CatboxClient
from discord_webhook import DiscordWebhook

import minecraft_modpack_auto_translator
from minecraft_modpack_auto_translator import create_resourcepack
from minecraft_modpack_auto_translator.config import (
    DICTIONARY_PREFIX_WHITELIST,
    DICTIONARY_SUFFIX_BLACKLIST,
    DIR_FILTER_WHITELIST,
)
from minecraft_modpack_auto_translator.finger_print import fingerprint_file
from minecraft_modpack_auto_translator.graph import (
    create_translation_graph,
    registry,
)
from minecraft_modpack_auto_translator.loaders.context import (
    TranslationContext,
)
from minecraft_modpack_auto_translator.translator import get_translator
from streamlit_utils import (
    extract_lang_content,
    get_delay_manager,
    get_rate_limiter,
    get_supported_extensions,
    initialize_translation_dictionary,
    load_custom_dictionary,
    render_api_key_management,
    render_custom_dictionary_upload,
    render_log_settings,
    render_model_provider_selection,
    render_model_selection,
    render_rate_limiter_settings,
    render_request_delay_settings,
    setup_logging,
)

catbox_client = CatboxClient(userhash=os.getenv("CATBOX_USERHASH"))

st.set_page_config(
    page_title="ëª¨ë“œíŒ© ë²ˆì—­ê¸°",
    page_icon="ğŸŒ",
    layout="wide",
)

logger = logging.getLogger(__name__)
# ë””ë²„ê·¸ ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)

# ì–¸ì–´ ì½”ë“œ ì„¤ì •
# .env íŒŒì¼ì—ì„œ ì–¸ì–´ ì½”ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ "ko_kr"ì…ë‹ˆë‹¤.
LANG_CODE = os.getenv("LANG_CODE", "ko_kr")

# API í‚¤ í™˜ê²½ ë³€ìˆ˜ ì´ë¦„ ë§¤í•‘
API_KEY_ENV_VARS = {
    "OpenAI": "OPENAI_API_KEY",
    "Google": "GOOGLE_API_KEY",
    "Grok": "GROK_API_KEY",
    "Ollama": "OLLAMA_API_KEY",
    "Anthropic": "ANTHROPIC_API_KEY",
}

# API ë² ì´ìŠ¤ URL í™˜ê²½ ë³€ìˆ˜ ì´ë¦„ ë§¤í•‘
API_BASE_ENV_VARS = {
    "OpenAI": "OPENAI_API_BASE",
    "Google": "GOOGLE_API_BASE",
    "Grok": "GROK_API_BASE",
    "Ollama": "OLLAMA_API_BASE",
    "Anthropic": "ANTHROPIC_API_BASE",
}


def get_parser_by_extension(extension):
    """íŒŒì¼ í™•ì¥ìì— ë§ëŠ” íŒŒì„œ í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    from minecraft_modpack_auto_translator.parsers.base_parser import BaseParser

    return BaseParser.get_parser_by_extension(extension)


def add_to_dictionary(
    en_value, ko_value, translation_dictionary, translation_dictionary_lowercase
):
    try:
        if en_value.lower() in translation_dictionary_lowercase:
            target = translation_dictionary[
                translation_dictionary_lowercase[en_value.lower()]
            ]
            if isinstance(target, list):
                if ko_value not in target:
                    translation_dictionary[
                        translation_dictionary_lowercase[en_value.lower()]
                    ].append(ko_value)
            elif isinstance(target, str):
                if (
                    translation_dictionary[
                        translation_dictionary_lowercase[en_value.lower()]
                    ]
                    != ko_value
                ):
                    translation_dictionary[
                        translation_dictionary_lowercase[en_value.lower()]
                    ] = [
                        translation_dictionary[
                            translation_dictionary_lowercase[en_value.lower()]
                        ],
                        ko_value,
                    ]
            else:
                st.error(
                    f"translation_dictionary[{en_value.lower()}]ì˜ íƒ€ì…ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {type(translation_dictionary[en_value.lower()])}"
                )
        else:
            translation_dictionary[en_value] = ko_value
            translation_dictionary_lowercase[en_value.lower()] = en_value

        return translation_dictionary, translation_dictionary_lowercase
    except Exception:
        logger.error(f"ë²ˆì—­ ì‚¬ì „ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {en_value}, {ko_value}")
        error_traceback = traceback.format_exc()
        logger.error(error_traceback)
        return translation_dictionary, translation_dictionary_lowercase


def build_dictionary_from_files(
    en_us_files,
    modpack_path,
    translation_dictionary,
    translation_dictionary_lowercase,
    source_lang_code,
):
    """ì£¼ì–´ì§„ ì›ë³¸ ì–¸ì–´ íŒŒì¼ê³¼ í•´ë‹¹í•˜ëŠ” ëª©í‘œ ì–¸ì–´ íŒŒì¼ì—ì„œ ë²ˆì—­ ì‚¬ì „ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."""

    file_count = 0
    entries_added = 0

    for en_file in en_us_files:
        try:
            # ëª©í‘œ ì–¸ì–´ íŒŒì¼ ê²½ë¡œ ì¶”ì •
            rel_path = en_file.replace(modpack_path, "").lstrip("/\\")
            target_lang_file = os.path.join(
                modpack_path,
                rel_path.replace(source_lang_code, LANG_CODE, 1),  # ì²« ë²ˆì§¸ ë°œìƒë§Œ êµì²´
            )

            # ëª©í‘œ ì–¸ì–´ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            if os.path.exists(target_lang_file):
                # íŒŒì¼ ë‚´ìš© ë¡œë“œ
                en_data = extract_lang_content(en_file)
                ko_data = extract_lang_content(
                    target_lang_file
                )  # ë³€ìˆ˜ëª…ì€ ko_data ìœ ì§€

                if not isinstance(en_data, dict) or not isinstance(ko_data, dict):
                    continue

                # ë²ˆì—­ ì‚¬ì „ì— ì¶”ê°€
                for key, en_value in en_data.items():
                    if (
                        key in ko_data
                        and isinstance(en_value, str)
                        and isinstance(ko_data[key], str)
                    ):
                        ko_value = ko_data[key]

                        # ë™ì¼í•œ ê°’ì´ë©´ ê±´ë„ˆë›°ê¸°
                        if en_value == ko_value:
                            continue

                        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸/ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
                        if (
                            key.split(".")[0] in DICTIONARY_PREFIX_WHITELIST
                            and key.split(".")[-1] not in DICTIONARY_SUFFIX_BLACKLIST
                        ):
                            # ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
                            clean_en = en_value.replace("_", "")
                            clean_ko = ko_value.replace("_", "")

                            translation_dictionary, translation_dictionary_lowercase = (
                                add_to_dictionary(
                                    clean_en,
                                    clean_ko,
                                    translation_dictionary,
                                    translation_dictionary_lowercase,
                                )
                            )
                            entries_added += 1

                file_count += 1

        except Exception as e:
            error_traceback = traceback.format_exc()
            st.error(
                f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}\n\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì½˜ì†” ì°½ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”."
            )
            logger.error(error_traceback)

    return (
        translation_dictionary,
        translation_dictionary_lowercase,
        file_count,
        entries_added,
    )


def build_dictionary_from_jar(
    jar_files,
    translation_dictionary,
    translation_dictionary_lowercase,
    source_lang_code,
):
    """JAR íŒŒì¼ ë‚´ë¶€ì˜ ì–¸ì–´ íŒŒì¼ì—ì„œ ë²ˆì—­ ì‚¬ì „ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."""

    file_count = 0
    entries_added = 0
    supported_extensions = get_supported_extensions()

    for jar_path in jar_files:
        try:
            with zipfile.ZipFile(jar_path, "r") as jar:
                # ì›ë³¸ ì–¸ì–´ íŒŒì¼ ì°¾ê¸°
                source_lang_files = [
                    f
                    for f in jar.namelist()
                    if os.path.splitext(f)[1] in supported_extensions
                    and (
                        source_lang_code.lower() in f.lower()
                    )  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì–¸ì–´ ì½”ë“œ ì‚¬ìš©
                ]

                for en_file in source_lang_files:
                    # ëª©í‘œ ì–¸ì–´ íŒŒì¼ ê²½ë¡œ ì¶”ì •
                    target_lang_file = en_file.replace(
                        source_lang_code, LANG_CODE, 1
                    )  # ì²« ë²ˆì§¸ ë°œìƒë§Œ êµì²´

                    # ë‘ íŒŒì¼ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                    if target_lang_file in jar.namelist():
                        try:
                            # íŒŒì¼ ë‚´ìš© ë¡œë“œ
                            with jar.open(en_file, "r") as f:
                                file_bytes = f.read()
                                en_content = file_bytes.decode("utf-8", errors="ignore")

                            with jar.open(
                                target_lang_file, "r"
                            ) as f:  # ë³€ìˆ˜ëª…ì€ ko_content ìœ ì§€
                                file_bytes = f.read()
                                ko_content = file_bytes.decode("utf-8", errors="ignore")

                            # íŒŒì„œë¡œ íŒŒì‹±
                            file_ext = os.path.splitext(en_file)[1]
                            parser_class = get_parser_by_extension(file_ext)

                            if parser_class:
                                en_data = parser_class.load(en_content)
                                ko_data = parser_class.load(ko_content)

                                # ë²ˆì—­ ì‚¬ì „ì— ì¶”ê°€
                                for key, en_value in en_data.items():
                                    if (
                                        key in ko_data
                                        and isinstance(en_value, str)
                                        and isinstance(ko_data[key], str)
                                    ):
                                        ko_value = ko_data[key]

                                        # ë™ì¼í•œ ê°’ì´ë©´ ê±´ë„ˆë›°ê¸°
                                        if en_value == ko_value:
                                            continue

                                        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸/ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
                                        if (
                                            key.split(".")[0]
                                            in DICTIONARY_PREFIX_WHITELIST
                                            and key.split(".")[-1]
                                            not in DICTIONARY_SUFFIX_BLACKLIST
                                        ):
                                            # ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
                                            clean_en = en_value.replace("_", "")
                                            clean_ko = ko_value.replace("_", "")

                                            (
                                                translation_dictionary,
                                                translation_dictionary_lowercase,
                                            ) = add_to_dictionary(
                                                clean_en,
                                                clean_ko,
                                                translation_dictionary,
                                                translation_dictionary_lowercase,
                                            )
                                            entries_added += 1

                                file_count += 1

                        except Exception as e:
                            st.error(
                                f"JAR íŒŒì¼ ë‚´ë¶€ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {jar_path}, {en_file}, {str(e)}\n\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì½˜ì†” ì°½ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”."
                            )
                            error_traceback = traceback.format_exc()
                            logger.error(error_traceback)

        except Exception as e:
            st.error(
                f"JAR íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {jar_path}, {str(e)}\n\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì½˜ì†” ì°½ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”."
            )
            error_traceback = traceback.format_exc()
            logger.error(error_traceback)

    return (
        translation_dictionary,
        translation_dictionary_lowercase,
        file_count,
        entries_added,
    )


# ê²½ë¡œ íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬ ë° ì •ê·œí™”
def normalize_glob_path(path):
    """
    glob íŒ¨í„´ì—ì„œ ì‚¬ìš©í•  ê²½ë¡œë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤.
    ê²½ë¡œ êµ¬ë¶„ìë¥¼ í†µì¼í•˜ê³  íŠ¹ìˆ˜ ë¬¸ìê°€ ìˆëŠ” ë¶€ë¶„ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    # ê²½ë¡œ êµ¬ë¶„ì í†µì¼ (ë°±ìŠ¬ë˜ì‹œ -> ìŠ¬ë˜ì‹œ)
    normalized_path = path.replace("\\", "/")

    # ì™€ì¼ë“œì¹´ë“œ ìˆëŠ”ì§€ í™•ì¸
    has_wildcard = "*" in normalized_path or "?" in normalized_path

    if has_wildcard:
        # ê²½ë¡œì™€ íŒ¨í„´ ë¶€ë¶„ ë¶„ë¦¬
        if "**" in normalized_path:
            # ì¬ê·€ì  íŒ¨í„´ ì²˜ë¦¬
            path_parts = normalized_path.split("/**", 1)
            base_dir = path_parts[0]
            pattern = "/**" + (path_parts[1] if len(path_parts) > 1 else "")
            # base_dir ë¶€ë¶„ë§Œ ì´ìŠ¤ì¼€ì´í”„
            return glob_escape(base_dir) + pattern
        else:
            # ì¼ë°˜ ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´
            last_wildcard_idx = max(
                normalized_path.rfind("*"), normalized_path.rfind("?")
            )
            if last_wildcard_idx != -1:
                last_dir_sep = normalized_path.rfind("/", 0, last_wildcard_idx)
                if last_dir_sep != -1:
                    # ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ ë¶€ë¶„ë§Œ ì´ìŠ¤ì¼€ì´í”„
                    return (
                        glob_escape(normalized_path[:last_dir_sep])
                        + normalized_path[last_dir_sep:]
                    )

    # ì™€ì¼ë“œì¹´ë“œê°€ ì—†ìœ¼ë©´ ì „ì²´ ê²½ë¡œ ì´ìŠ¤ì¼€ì´í”„
    return glob_escape(normalized_path)


def process_modpack_directory(
    modpack_path,
    source_lang_code,
    translate_config=True,
    translate_kubejs=True,
    translate_mods=True,
):
    """ëª¨ë“œíŒ© ë””ë ‰í† ë¦¬ì—ì„œ ë²ˆì—­ ëŒ€ìƒ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤."""
    supported_extensions = get_supported_extensions()
    source_lang_code_lower = source_lang_code.lower()  # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©

    # ë²ˆì—­ ëŒ€ìƒ íŒŒì¼ ê²€ìƒ‰
    source_lang_files = []

    # config í´ë” ë‚´ íŒŒì¼ ê²€ìƒ‰ (ì„ íƒí•œ ê²½ìš°)
    if translate_config:
        config_glob_path = normalize_glob_path(
            os.path.join(modpack_path, "config/**/*.*")
        )
        config_files = glob(config_glob_path, recursive=True)
        for f in config_files:
            f = f.replace("\\", "/")
            file_ext = os.path.splitext(f)[1]
            if file_ext in supported_extensions and any(
                whitelist_dir in f for whitelist_dir in DIR_FILTER_WHITELIST
            ):
                source_lang_files.append(f)
            elif file_ext in supported_extensions and (
                source_lang_code_lower in f.lower()
            ):  # ì‚¬ìš©ì ì…ë ¥ ì–¸ì–´ ì½”ë“œ í™•ì¸
                source_lang_files.append(f)

    # kubejs í´ë” ë‚´ íŒŒì¼ ê²€ìƒ‰ (ì„ íƒí•œ ê²½ìš°)
    if translate_kubejs:
        kubejs_glob_path = normalize_glob_path(
            os.path.join(modpack_path, "kubejs/**/*.*")
        )
        kubejs_files = glob(kubejs_glob_path, recursive=True)
        for f in kubejs_files:
            f = f.replace("\\", "/")
            file_ext = os.path.splitext(f)[1]
            if file_ext in supported_extensions and any(
                whitelist_dir in f for whitelist_dir in DIR_FILTER_WHITELIST
            ):
                source_lang_files.append(f)
            elif file_ext in supported_extensions and (
                source_lang_code_lower in f.lower()
            ):  # ì‚¬ìš©ì ì…ë ¥ ì–¸ì–´ ì½”ë“œ í™•ì¸
                source_lang_files.append(f)

    # mods í´ë” ë‚´ jar íŒŒì¼ ê²€ìƒ‰ (ì„ íƒí•œ ê²½ìš°)
    mods_jar_files = []
    jar_files_fingerprint = {}
    if translate_mods:
        mods_glob_path = normalize_glob_path(os.path.join(modpack_path, "mods/*.jar"))
        mods_jar_files = glob(mods_glob_path)

        extract_dir = os.path.join(modpack_path, "extracted")
        os.makedirs(extract_dir, exist_ok=True)

        for jar_path in mods_jar_files:
            try:
                jar_files_fingerprint[os.path.basename(jar_path)] = fingerprint_file(
                    jar_path
                )
                with zipfile.ZipFile(jar_path, "r") as jar:
                    # ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ ì°¾ê¸°
                    lang_files = [
                        f
                        for f in jar.namelist()
                        if os.path.splitext(f)[1] in supported_extensions
                        and (
                            any(
                                whitelist_dir in f
                                for whitelist_dir in DIR_FILTER_WHITELIST
                            )
                            or (
                                source_lang_code_lower in f.lower()
                            )  # ì‚¬ìš©ì ì…ë ¥ ì–¸ì–´ ì½”ë“œ í™•ì¸
                        )
                    ]

                    for lang_file in lang_files:
                        # ì„ì‹œ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì¶”ì¶œ
                        extract_path = os.path.join(
                            extract_dir, os.path.basename(jar_path), lang_file
                        ).replace("\\", "/")
                        os.makedirs(os.path.dirname(extract_path), exist_ok=True)

                        with (
                            jar.open(lang_file) as source,
                            open(extract_path, "wb") as target,
                        ):
                            shutil.copyfileobj(source, target)

                        source_lang_files.append(extract_path)
            except Exception as e:
                st.error(
                    f"JAR íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}, {jar_path}\n\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì½˜ì†” ì°½ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”."
                )
                error_traceback = traceback.format_exc()
                logger.error(error_traceback)

    return source_lang_files, mods_jar_files, jar_files_fingerprint


def main():
    st.title("ğŸŒ ì›í´ë¦­ ëª¨ë“œíŒ© ë²ˆì—­ê¸°")

    # ê¸€ë¡œë²Œ API í‚¤ ì¸ë±ìŠ¤ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
    if "api_key_index" not in st.session_state:
        st.session_state.api_key_index = 0

    # ì‚¬ì´ë“œë°”ì— ëª¨ë¸ ì„ íƒ ì˜µì…˜
    st.sidebar.header("ë²ˆì—­ ì„¤ì •")

    model_provider = render_model_provider_selection()
    api_keys = render_api_key_management(model_provider)
    selected_model, api_base_url, temperature = render_model_selection(model_provider)
    use_rate_limiter, rpm = render_rate_limiter_settings(model_provider)
    use_request_delay, request_delay = render_request_delay_settings(model_provider)

    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    st.sidebar.subheader("ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •")
    if model_provider == "G4F":
        max_workers = 5  # G4Fì— ì í•©í•œ ë™ì‹œ ì‘ì—…ì ìˆ˜
        file_split_number = 3  # G4Fì—ì„œëŠ” íŒŒì¼ ë¶„í•  ë¹„í™œì„±í™”
        st.sidebar.markdown(
            "G4F ëª¨ë“œ: ë™ì‹œ ì‘ì—…ì ìˆ˜ ê³ ì • (5ëª…)  \nG4F ëª¨ë“œ: íŒŒì¼ ë¶„í•  ì‘ì—…ì ìˆ˜ ê³ ì • (3ê°œ)"
        )
    else:
        max_workers = st.sidebar.number_input(
            "ë™ì‹œ ì‘ì—…ì ìˆ˜",
            min_value=1,
            max_value=100,
            value=5,
            step=1,
            help="ë™ì‹œì— ì²˜ë¦¬í•  ë²ˆì—­ ì‘ì—… ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ê°’ì´ ë†’ì„ìˆ˜ë¡ ë²ˆì—­ ì†ë„ê°€ ë¹¨ë¼ì§€ì§€ë§Œ, API í• ë‹¹ëŸ‰ì„ ë¹ ë¥´ê²Œ ì†Œëª¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ìˆ«ìëŠ” ë†’ì„ìˆ˜ë¡ ëª‡ê°œì˜ íŒŒì¼ì„ ë™ì‹œì— ì—´ê³  ì‘ì—…í• ì§€ë¥¼ ì„¤ì • í•©ë‹ˆë‹¤.",
        )

        file_split_number = st.sidebar.number_input(
            "íŒŒì¼ ë¶„í•  ì‘ì—…ì ìˆ˜",
            min_value=1,
            max_value=100,
            value=1,
            step=1,
            help="íŒŒì¼ ë¶„í•  ì‘ì—…ì ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ê°’ì´ ë†’ì„ìˆ˜ë¡ í•œê°œì˜ íŒŒì¼ì„ nê°œë¡œ ë¶„í• í•˜ì—¬ ì‘ì—…í•˜ì—¬ ì†ë„ê°€ ë¹¨ë¼ì§‘ë‹ˆë‹¤. í•˜ì§€ë§Œ 1ë³´ë‹¤ í¬ê²Œ ì„¤ì •í•œë‹¤ë©´ ì‚¬ì „ì„ ë™ì‹œì— ì‘ì„±í•˜ë©´ì„œ ê°™ì€ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ëœë¤ ìˆœì„œ ì‚¬ìš© ê¶Œì¥ì¥)",
        )

    use_random_order = st.sidebar.checkbox(
        "ëœë¤ ìˆœì„œë¡œ ë²ˆì—­",
        value=False,
        help="íŒŒì¼ ë‚´ë¶€ í•­ëª©ì„ ëœë¤ ìˆœì„œë¡œ ë²ˆì—­í•˜ì—¬ ë³‘ë ¬ ë²ˆì—­ ì‹œ ì‚¬ì „ì˜ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.",
        key="random_order_checkbox",
    )

    # UI ì—…ë°ì´íŠ¸ ì„¤ì •
    st.sidebar.subheader("UI ì„¤ì •")
    update_interval = st.sidebar.slider(
        "ì—…ë°ì´íŠ¸ ê°„ê²©(ì´ˆ)",
        min_value=1.0,
        max_value=10.0,
        value=3.0,
        step=0.5,
        help="UIê°€ ì—…ë°ì´íŠ¸ë˜ëŠ” ê°„ê²©ì„ ì„¤ì •í•©ë‹ˆë‹¤. ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì‹¤ì‹œê°„ìœ¼ë¡œ ì •ë³´ê°€ ê°±ì‹ ë˜ì§€ë§Œ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    )

    max_log_lines = render_log_settings()
    custom_dict_file = render_custom_dictionary_upload()

    # ëª¨ë“œíŒ© ì„ íƒ
    # í´ë” ì„ íƒ (ì‹¤ì œë¡œëŠ” í´ë” ê²½ë¡œ ì…ë ¥) -> íŒŒì¼ ì—…ë¡œë“œë¡œ ë³€ê²½
    with st.container(border=True):
        st.subheader("ğŸ“Œ ì—…ë¡œë“œí•  ZIP íŒŒì¼ ì•ˆë‚´")
        st.markdown("""
        - **ë§ˆì¸í¬ë˜í”„íŠ¸ ëª¨ë“œíŒ© ZIP íŒŒì¼**ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”  
        - ì¼ë°˜ì ì¸ `.minecraft` í´ë”ë¥¼ ì••ì¶•í•œ ZIP íŒŒì¼ì„ ì˜ë¯¸í•©ë‹ˆë‹¤  
        - ì£¼ë¡œ í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ” í´ë”:  
          `ğŸ“ mods` `ğŸ“ config` `ğŸ“ kubeJS`  
        - âš ï¸ ì„œë²„íŒ©ì´ ì•„ë‹Œ **í´ë¼ì´ì–¸íŠ¸ ëª¨ë“œíŒ©**ì„ ì¶”ì²œí•©ë‹ˆë‹¤  
        - ëª¨ë“œíŒ© ìš©ëŸ‰ì´ 1GB ì´ˆê³¼ì‹œ:  
          `mods`, `kubejs`, `config` í´ë”ë§Œ ì••ì¶•í•´ì£¼ì„¸ìš” or ë”°ë¡œ ë”°ë¡œ ë²ˆì—­ ì§„í–‰í•˜ë©° ì»¤ìŠ¤í…€ ì‚¬ì „ ê¸°ëŠ¥ í™œìš©
        """)

    want_to_share_result = st.checkbox(
        "ë²ˆì—­ ê²°ê³¼ ê³µìœ ",
        value=True,
        help="ê³µì‹ ë””ìŠ¤ì½”ë“œì— ë²ˆì—­ ê²°ê³¼ë¥¼ ê³µìœ í•©ë‹ˆë‹¤. (ë¹…ë°ì´í„°ê°€ ìŒ“ì´ë©´ í° í˜ì´ ë©ë‹ˆë‹¤.)",
    )
    # ì›ë³¸ ì–¸ì–´ ì½”ë“œ ì…ë ¥
    source_lang_code = st.text_input(
        "ì›ë³¸ ì–¸ì–´ ì½”ë“œ",
        "en_us",
        placeholder="ë²ˆì—­í•  ì›ë³¸ ì–¸ì–´ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: en_us)",
    ).lower()  # ì…ë ¥ê°’ì„ ì†Œë¬¸ìë¡œ ë³€í™˜

    uploaded_file = st.file_uploader("ëª¨ë“œíŒ© ZIP íŒŒì¼ ì—…ë¡œë“œ", type=["zip"])

    # ë²ˆì—­ ê²°ê³¼, ê¸°ì¡´ ë²ˆì—­ ìë™ ì‚¬ì „ êµ¬ì¶• ì˜µì…˜
    build_dict_from_existing = st.checkbox("ê¸°ì¡´ ë²ˆì—­ì—ì„œ ì‚¬ì „ ìë™ êµ¬ì¶•", value=True)

    # ì˜µì…˜: ì´ë¯¸ ë²ˆì—­ëœ íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°
    skip_translated = st.checkbox("ì´ë¯¸ ë²ˆì—­ëœ íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°", value=True)

    # ë¦¬ì†ŒìŠ¤íŒ© ì´ë¦„ ì„¤ì •
    resourcepack_name = st.text_input("ë¦¬ì†ŒìŠ¤íŒ© ì´ë¦„", "Auto-Translated-KO")

    # ë²ˆì—­ ì¹´í…Œê³ ë¦¬ ì„ íƒ
    st.subheader("ë²ˆì—­ ì¹´í…Œê³ ë¦¬ ì„ íƒ")
    translate_config = st.checkbox("Config íŒŒì¼ ë²ˆì—­", value=True)
    translate_kubejs = st.checkbox("KubeJS íŒŒì¼ ë²ˆì—­", value=True)
    translate_mods = st.checkbox("Mods íŒŒì¼ ë²ˆì—­", value=True)

    # ì»¤ìŠ¤í…€ ì‚¬ì „ ì²˜ë¦¬
    translation_dictionary = {}
    translation_dictionary_lowercase = {}

    target_lang_code = LANG_CODE
    # ì‚¬ì „ ì´ˆê¸°í™” ë° ë¡œë“œ (streamlit_utils ì‚¬ìš©)
    translation_dictionary, translation_dictionary_lowercase = (
        initialize_translation_dictionary(source_lang_code, target_lang_code)
    )
    translation_dictionary, translation_dictionary_lowercase = load_custom_dictionary(
        custom_dict_file, translation_dictionary, translation_dictionary_lowercase
    )

    # ë²ˆì—­ ì‹¤í–‰ ë²„íŠ¼
    if st.button("ë²ˆì—­ ì‹œì‘"):
        if not api_keys and model_provider != "G4F":
            st.error("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        # ZIP íŒŒì¼ ì—…ë¡œë“œ í™•ì¸
        if uploaded_file is None:
            st.error("ëª¨ë“œíŒ© ZIP íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return

        # ìµœì†Œí•œ í•˜ë‚˜ì˜ ì¹´í…Œê³ ë¦¬ëŠ” ì„ íƒë˜ì–´ì•¼ í•¨
        if not (translate_config or translate_kubejs or translate_mods):
            st.error("ìµœì†Œí•œ í•˜ë‚˜ì˜ ë²ˆì—­ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return

        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_input_dir = os.path.join(temp_dir, "input").replace("\\", "/")
            temp_output_dir = os.path.join(
                temp_dir, "output", resourcepack_name
            ).replace("\\", "/")  # ë¦¬ì†ŒìŠ¤íŒ© ì´ë¦„ í¬í•¨

            os.makedirs(temp_input_dir, exist_ok=True)
            os.makedirs(temp_output_dir, exist_ok=True)

            # ì—…ë¡œë“œëœ ZIP íŒŒì¼ ì••ì¶• í•´ì œ
            try:
                with zipfile.ZipFile(uploaded_file, "r") as zip_ref:
                    zip_ref.extractall(temp_input_dir)
                st.info(f"'{uploaded_file.name}' íŒŒì¼ ì••ì¶• í•´ì œ ì™„ë£Œ.")
            except Exception as e:
                st.error(f"ZIP íŒŒì¼ ì••ì¶• í•´ì œ ì¤‘ ì˜¤ë¥˜: {e}")
                logger.error(traceback.format_exc())
                return

            # ê²½ë¡œ ë³€ìˆ˜ ì„¤ì •
            modpack_path = temp_input_dir
            output_path = (
                temp_output_dir  # ì´ì œ output_pathëŠ” ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ ê°€ë¦¬í‚´
            )

            # ----- ë²ˆì—­ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ -----
            log_session_key = "main_translation_logs"  # ê³ ìœ í•œ í‚¤ ì‚¬ìš© ê¶Œì¥
            log_handler = setup_logging(
                max_log_lines=max_log_lines, session_key=log_session_key
            )

            # ë¡œê·¸ ì„¸ì…˜ ìƒíƒœ í‚¤ ëª…ì‹œì  ì´ˆê¸°í™” (KeyError ë°©ì§€)
            if log_session_key not in st.session_state:
                st.session_state[log_session_key] = []

            # ë¡œê·¸ë¥¼ í‘œì‹œí•  UI ì˜ì—­ ìƒì„± (st.expander ì‚¬ìš© ì˜ˆì‹œ)
            log_container = st.expander("ë²ˆì—­ ë¡œê·¸", expanded=True)
            with log_container:
                # st.session_stateì—ì„œ ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì™€ í‘œì‹œ
                # ì´ì œ .get() ëŒ€ì‹  ì§ì ‘ ì ‘ê·¼í•´ë„ ì•ˆì „í•©ë‹ˆë‹¤.
                log_messages_to_display = st.session_state[log_session_key]
                # ë¡œê·¸ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ í‘œì‹œ (ì¤„ë°”ê¿ˆ '\n' ëŒ€ì‹  Markdown ì¤„ë°”ê¿ˆ '  \n' ì‚¬ìš©)
                st.markdown(
                    "  \n".join(log_messages_to_display), unsafe_allow_html=True
                )

                # ë¡œê·¸ ì§€ìš°ê¸° ë²„íŠ¼ (ì„ íƒ ì‚¬í•­)
                if st.button("ë¡œê·¸ ì§€ìš°ê¸°"):
                    if log_handler:
                        log_handler.clear_logs()
                        # UI ì¦‰ì‹œ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ rerun
                        st.rerun()

            try:
                with st.spinner("ë²ˆì—­ ì§„í–‰ ì¤‘..."):
                    # ì „ì²´ ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ ìƒíƒœ í‘œì‹œ ë°”ì™€ ì •ë³´ í‘œì‹œ ì˜ì—­
                    st.subheader("ì „ì²´ ì§„í–‰ ìƒí™©")
                    progress_cols = st.columns([4, 1])
                    with progress_cols[0]:
                        overall_progress_bar = st.progress(0)
                    with progress_cols[1]:
                        overall_progress_text = st.empty()
                    status_text = st.empty()

                    # ì‘ì—…ìë³„ ì§„í–‰ ìƒí™© ì»¨í…Œì´ë„ˆ
                    worker_progress_bars = {}
                    worker_progress_texts = {}
                    worker_statuses = {}

                    # ì‘ì—…ìë³„ ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
                    for i in range(max_workers):
                        st.markdown(f"### Worker {i + 1}")
                        worker_cols = st.columns([3, 1])

                        with worker_cols[0]:
                            worker_progress_bars[i] = st.progress(0)
                        with worker_cols[1]:
                            worker_progress_texts[i] = st.empty()

                        worker_statuses[i] = {
                            "active": False,
                            "file": "",
                            "progress": 0,
                        }
                        st.markdown("---")

                    # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                    status_text.text("ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
                    logger.info("ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")  # ì´ì œ ì •ìƒ í˜¸ì¶œ ê°€ëŠ¥

                    # ê¸€ë¡œë²Œ API í‚¤ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
                    st.session_state.api_key_index = 0
                    total_api_keys = len(api_keys)

                    logger.info(
                        f"ì´ {total_api_keys}ê°œì˜ API í‚¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
                    )

                    # Rate Limiter ë° Delay Manager ìƒì„± (streamlit_utils ì‚¬ìš©)
                    rate_limiter = get_rate_limiter(
                        use_rate_limiter and model_provider != "G4F", rpm
                    )
                    if rate_limiter:
                        logger.info(f"ì†ë„ ì œí•œ ì„¤ì •: {rpm} RPM ({rpm / 60.0:.2f} RPS)")

                    g4f_delay = 1.0 if model_provider == "G4F" else 0
                    effective_delay = (
                        request_delay
                        if use_request_delay and model_provider != "G4F"
                        else g4f_delay
                    )
                    delay_manager = get_delay_manager(
                        effective_delay > 0, effective_delay
                    )
                    if effective_delay > 0:
                        logger.info(f"ìš”ì²­ ë”œë ˆì´ ì„¤ì •: {effective_delay:.1f}ì´ˆ")

                    st.session_state.api_key_index = (
                        st.session_state.api_key_index + 1
                    ) % total_api_keys

                    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì„ì‹œ ë””ë ‰í† ë¦¬ ë‚´ë¶€ì—)
                    os.makedirs(output_path, exist_ok=True)
                    dictionary_path = os.path.join(output_path, "dictionary")
                    os.makedirs(dictionary_path, exist_ok=True)
                    logger.info(f"ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: {output_path}")

                    # UUID ìƒì„± (ë¦¬ì†ŒìŠ¤íŒ© ì‹ë³„ìë¡œ ì‚¬ìš©)
                    uuid_str = str(uuid.uuid4())

                    # ëª¨ë“œíŒ© ë””ë ‰í† ë¦¬ì—ì„œ ë²ˆì—­í•  íŒŒì¼ ì°¾ê¸°
                    status_text.text("ë²ˆì—­ ëŒ€ìƒ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
                    logger.info("ë²ˆì—­ ëŒ€ìƒ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
                    source_lang_files, mods_jar_files, jar_files_fingerprint = (
                        process_modpack_directory(
                            modpack_path,
                            source_lang_code,
                            translate_config,
                            translate_kubejs,
                            translate_mods,
                        )
                    )
                    with open(
                        os.path.join(output_path, "jar_files_fingerprint.json"),
                        "w",
                        encoding="utf-8",
                    ) as f:
                        json.dump(
                            jar_files_fingerprint, f, ensure_ascii=False, indent=4
                        )

                    if len(source_lang_files) == 0:
                        logger.warning("ë²ˆì—­í•  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.warning("ë²ˆì—­í•  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return

                    status_text.text(
                        f"{len(source_lang_files)}ê°œì˜ ì–¸ì–´ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
                    )
                    logger.info(f"{len(source_lang_files)}ê°œì˜ ì–¸ì–´ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

                    # ê¸°ì¡´ ë²ˆì—­ì—ì„œ ì‚¬ì „ êµ¬ì¶•
                    if build_dict_from_existing:
                        status_text.text("ê¸°ì¡´ ë²ˆì—­ì—ì„œ ì‚¬ì „ êµ¬ì¶• ì¤‘...")
                        logger.info("ê¸°ì¡´ ë²ˆì—­ì—ì„œ ì‚¬ì „ êµ¬ì¶• ì¤‘...")

                        # JAR íŒŒì¼ì—ì„œ ì‚¬ì „ êµ¬ì¶•
                        (
                            translation_dictionary,
                            translation_dictionary_lowercase,
                            jar_files_count,
                            jar_entries_added,
                        ) = build_dictionary_from_jar(
                            mods_jar_files,
                            translation_dictionary,
                            translation_dictionary_lowercase,
                            source_lang_code,
                        )
                        logger.info(
                            f"JAR íŒŒì¼ {jar_files_count}ê°œì—ì„œ {jar_entries_added}ê°œ í•­ëª© ì¶”ê°€"
                        )

                        # ì¼ë°˜ íŒŒì¼ì—ì„œ ì‚¬ì „ êµ¬ì¶•
                        (
                            translation_dictionary,
                            translation_dictionary_lowercase,
                            files_count,
                            entries_added,
                        ) = build_dictionary_from_files(
                            source_lang_files,
                            modpack_path,
                            translation_dictionary,
                            translation_dictionary_lowercase,
                            source_lang_code,
                        )
                        logger.info(
                            f"ì¼ë°˜ íŒŒì¼ {files_count}ê°œì—ì„œ {entries_added}ê°œ í•­ëª© ì¶”ê°€"
                        )

                        # ì‚¬ì „ ì •ë³´ í‘œì‹œ
                        total_files = jar_files_count + files_count
                        total_entries = jar_entries_added + entries_added
                        logger.info(
                            f"ì´ {total_files}ê°œ íŒŒì¼ì—ì„œ {total_entries}ê°œ í•­ëª©ì„ ì‚¬ì „ì— ì¶”ê°€",
                        )

                        logger.info(
                            f"ê¸°ì¡´ ë²ˆì—­ì—ì„œ {total_files}ê°œ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ {total_entries}ê°œ í•­ëª©ì„ ì‚¬ì „ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤."
                        )

                    # ê³µìœ  ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    shared_context = TranslationContext(
                        translation_graph=create_translation_graph(),
                        custom_dictionary_dict=translation_dictionary,
                        registry=registry,
                    )
                    shared_context.initialize_dictionaries()

                    try:
                        dict_size = len(shared_context.get_dictionary())
                        logger.info(
                            f"ê³µìœ  ë²ˆì—­ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {dict_size}ê°œ ì‚¬ì „ í•­ëª©",
                        )
                    except Exception as e:
                        logger.info(
                            f"ê³µìœ  ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì€ ì™„ë£Œëìœ¼ë‚˜ ì‚¬ì „ ì •ë³´ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}",
                            "warning",
                        )
                        logger.info("ë²ˆì—­ì€ ì •ìƒì ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.", "info")

                    status_text.text(
                        f"ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤... ({len(translation_dictionary)}ê°œ ì‚¬ì „ í•­ëª© ì‚¬ìš©)"
                    )
                    logger.info(
                        f"ë²ˆì—­ ì‹œì‘ ({len(translation_dictionary)}ê°œ ì‚¬ì „ í•­ëª© ì‚¬ìš©)",
                    )

                    # ë²ˆì—­ íŒŒì¼ ë§¤í•‘ (ì›ë³¸ -> ë²ˆì—­)
                    translated_files = {}

                    # ì˜¤ë¥˜ ë°œìƒ íŒŒì¼ ëª©ë¡
                    failed_files = []

                    # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
                    file_types = {"config": [], "kubejs": [], "mods": []}

                    for file_path in source_lang_files:
                        # tmp_ ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°
                        if os.path.basename(file_path).startswith("tmp_"):
                            continue

                        if "/config/" in file_path or "\\config\\" in file_path:
                            file_types["config"].append(file_path)
                        elif "/kubejs/" in file_path or "\\kubejs\\" in file_path:
                            file_types["kubejs"].append(file_path)
                        else:
                            file_types["mods"].append(file_path)

                    # ì„ íƒë˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬ í•„í„°ë§
                    if not translate_config:
                        file_types["config"] = []
                    if not translate_kubejs:
                        file_types["kubejs"] = []
                    if not translate_mods:
                        file_types["mods"] = []

                    # íŒŒì¼ íƒ€ì…ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
                    with open(
                        os.path.join(output_path, "processing_info.json"),
                        "w",
                        encoding="utf-8",
                    ) as f:
                        json.dump(file_types, f, ensure_ascii=False, indent=4)

                    # ì¹´í…Œê³ ë¦¬ë³„ ë²ˆì—­ ì§„í–‰
                    total_files = len(source_lang_files)
                    processed_files = 0

                    # ì‘ì—…ìë³„ ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
                    worker_statuses = {
                        i: {"active": False, "file": "", "progress": 0}
                        for i in range(max_workers)
                    }

                    # ë²ˆì—­ ì‹œì‘ ì‹œê°„ ê¸°ë¡
                    start_time = time.time()
                    last_update_time = {}  # ê° ì›Œì»¤ë³„ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„
                    processing_speeds = []  # ìµœê·¼ ì²˜ë¦¬ ì†ë„ ê¸°ë¡ (í•­ëª©/ì´ˆ)
                    max_speed_samples = 10  # ì†ë„ ê³„ì‚°ì— ì‚¬ìš©í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜

                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜
                    async def update_progress(
                        worker_id,
                        file_path=None,
                        progress=None,
                        done=False,
                        total_items=None,
                        processed_items=None,
                    ):
                        current_time = time.time()

                        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ í™•ì¸
                        if worker_id in last_update_time:
                            # ì—…ë°ì´íŠ¸ ê°„ê²©ì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì—…ë°ì´íŠ¸ ê±´ë„ˆë›°ê¸° (íŒŒì¼ ì™„ë£Œ ì‹œëŠ” ì œì™¸)
                            if (
                                not done
                                and current_time - last_update_time[worker_id]
                                < update_interval
                            ):
                                return

                        # í˜„ì¬ ì‹œê°„ì„ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ìœ¼ë¡œ ê¸°ë¡
                        last_update_time[worker_id] = current_time

                        # ê²½ê³¼ ì‹œê°„ ê³„ì‚°
                        elapsed_time = current_time - start_time
                        hours, remainder = divmod(elapsed_time, 3600)
                        minutes, seconds = divmod(remainder, 60)
                        elapsed_str = (
                            f"{int(hours):02}:{int(minutes):02}:{int(seconds):02}"
                        )

                        if file_path:
                            worker_statuses[worker_id]["file"] = file_path

                        if progress is not None:
                            worker_statuses[worker_id]["progress"] = progress

                        if done:
                            worker_statuses[worker_id]["active"] = False
                            nonlocal processed_files
                            processed_files += 1
                            overall_progress = int(
                                (processed_files / total_files) * 100
                            )
                            overall_progress_bar.progress(overall_progress)
                            overall_progress_text.markdown(
                                f"**{processed_files}/{total_files}** ({overall_progress}%)"
                            )
                        else:
                            worker_statuses[worker_id]["active"] = True

                        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚° (ì „ì²´ì˜ 1% ì´ìƒ ì²˜ë¦¬ëœ ê²½ìš°ì—ë§Œ)
                        if (
                            processed_files > 0
                            and processed_files < total_files
                            and elapsed_time > 10
                        ):
                            completion_percentage = processed_files / total_files
                            if completion_percentage > 0.01:
                                estimated_total_time = (
                                    elapsed_time / completion_percentage
                                )
                                remaining_time = estimated_total_time - elapsed_time

                                # ë‚¨ì€ ì‹œê°„ í˜•ì‹í™”
                                r_hours, r_remainder = divmod(remaining_time, 3600)
                                r_minutes, r_seconds = divmod(r_remainder, 60)
                                remaining_str = f"{int(r_hours):02}:{int(r_minutes):02}:{int(r_seconds):02}"

                                # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
                                import datetime

                                completion_time = (
                                    datetime.datetime.now()
                                    + datetime.timedelta(seconds=remaining_time)
                                )

                                # ì˜¤ëŠ˜ì´ë©´ ì‹œê°„ë§Œ, ë‚´ì¼ ì´í›„ë©´ ë‚ ì§œì™€ ì‹œê°„ í‘œì‹œ
                                today = datetime.datetime.now().date()
                                if completion_time.date() == today:
                                    completion_str = completion_time.strftime(
                                        "%H:%M:%S"
                                    )
                                    completion_display = f"ì˜¤ëŠ˜ {completion_str}"
                                elif (
                                    completion_time.date()
                                    == today + datetime.timedelta(days=1)
                                ):
                                    completion_str = completion_time.strftime(
                                        "%H:%M:%S"
                                    )
                                    completion_display = f"ë‚´ì¼ {completion_str}"
                                else:
                                    completion_str = completion_time.strftime(
                                        "%m/%d %H:%M:%S"
                                    )
                                    completion_display = completion_str

                                time_info = f"ê²½ê³¼: {elapsed_str} | ë‚¨ìŒ: {remaining_str} | ì™„ë£Œ ì˜ˆìƒ: {completion_display}"
                            else:
                                time_info = f"ê²½ê³¼: {elapsed_str}"
                        else:
                            time_info = f"ê²½ê³¼: {elapsed_str}"

                        # ì‘ì—…ì ìƒíƒœ ì—…ë°ì´íŠ¸
                        status_prefix = (
                            "ğŸŸ¢ Active"
                            if worker_statuses[worker_id]["active"]
                            else "âšª Waiting"
                        )

                        if total_items and processed_items is not None:
                            # ì²˜ë¦¬ ì†ë„ ê³„ì‚° (í•­ëª©/ì´ˆ)
                            if (
                                done
                                and "start_processing_time"
                                in worker_statuses[worker_id]
                            ):
                                processing_time = (
                                    current_time
                                    - worker_statuses[worker_id][
                                        "start_processing_time"
                                    ]
                                )
                                if processing_time > 0:
                                    items_per_second = total_items / processing_time
                                    processing_speeds.append(items_per_second)
                                    # ìµœê·¼ nê°œ ìƒ˜í”Œë§Œ ìœ ì§€
                                    if len(processing_speeds) > max_speed_samples:
                                        processing_speeds.pop(0)
                            elif (
                                not done
                                and "start_processing_time"
                                not in worker_statuses[worker_id]
                            ):
                                worker_statuses[worker_id]["start_processing_time"] = (
                                    current_time
                                )

                            item_progress = f"{processed_items}/{total_items} í•­ëª©"
                            worker_progress_texts[worker_id].markdown(
                                f"{status_prefix} - **{worker_statuses[worker_id]['file']}** ({item_progress})"
                            )
                        else:
                            worker_progress_texts[worker_id].markdown(
                                f"{status_prefix} - **{worker_statuses[worker_id]['file']}**"
                            )

                        worker_progress_bars[worker_id].progress(
                            worker_statuses[worker_id]["progress"]
                        )
                        worker_progress_texts[worker_id].markdown(
                            f"**{worker_statuses[worker_id]['progress']}%**"
                        )

                        # í‰ê·  ì²˜ë¦¬ ì†ë„ ê³„ì‚°
                        avg_speed = (
                            sum(processing_speeds) / len(processing_speeds)
                            if processing_speeds
                            else 0
                        )
                        speed_info = (
                            f"í‰ê·  ì†ë„: {avg_speed:.2f} í•­ëª©/ì´ˆ"
                            if avg_speed > 0
                            else "í‰ê·  ì†ë„: ê³„ì‚°ì¤‘.."
                        )

                        # ì „ì²´ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                        percent_complete = int((processed_files / total_files) * 100)
                        status_text.markdown(
                            f"ë²ˆì—­ ì¤‘... **{processed_files}/{total_files}** íŒŒì¼ ì™„ë£Œ ({percent_complete}%) - "
                            f"í™œì„± ì‘ì—…ì: {sum(1 for s in worker_statuses.values() if s['active'])}ëª…  \n"
                            f"â±ï¸ {time_info} | {speed_info} | ì—…ë°ì´íŠ¸ ê°„ê²©: {update_interval}ì´ˆ"
                        )

                    # ì‚¬ì „ ì •ë ¬ ë° í•„í„°ë§ í•¨ìˆ˜
                    def sort_and_filter_dictionary():
                        sorted_dict = {}
                        for k, v in sorted(
                            translation_dictionary.items(),
                            key=lambda item: len(item[0]),
                            reverse=True,
                        ):
                            if (
                                len(k.split(" ")) <= 10
                                and not re.search(r"[.?!%]", k)
                                and not re.search(
                                    r"[.?!%]",
                                    ", ".join(v) if isinstance(v, list) else v,
                                )
                            ):
                                if isinstance(v, list):
                                    sorted_dict[k] = v
                                else:
                                    sorted_dict[k] = v
                        return sorted_dict

                    # ê° ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬ ë””ë ‰í† ë¦¬ ìƒì„±
                    for category in ["config", "kubejs", "mods"]:
                        os.makedirs(
                            os.path.join(output_path, category, "input"), exist_ok=True
                        )
                        os.makedirs(
                            os.path.join(output_path, category, "output"), exist_ok=True
                        )

                    # ë‹¨ì¼ íŒŒì¼ ë²ˆì—­ í•¨ìˆ˜
                    async def translate_file(worker_id, en_file, category):
                        try:
                            # ì‘ì—…ì ìƒíƒœ ì—…ë°ì´íŠ¸
                            await update_progress(worker_id, en_file, 0)

                            # í˜„ì¬ API í‚¤ ê°€ì ¸ì˜¤ê¸°
                            current_api_key = api_keys[
                                st.session_state.api_key_index % total_api_keys
                            ]

                            # ë‹¤ìŒ API í‚¤ ì¸ë±ìŠ¤ë¡œ ì—…ë°ì´íŠ¸
                            st.session_state.api_key_index = (
                                st.session_state.api_key_index + 1
                            ) % total_api_keys

                            # íŒŒì¼ ì´ë¦„ì´ tmp_ë¡œ ì‹œì‘í•˜ë©´ ê±´ë„ˆë›°ê¸°
                            if os.path.basename(en_file).startswith("tmp_"):
                                await update_progress(worker_id, en_file, 100, True)
                                return

                            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
                            rel_path = en_file.replace(modpack_path, "").lstrip("/\\")

                            # ì¹´í…Œê³ ë¦¬ë³„ ì¶œë ¥ ê²½ë¡œ ì„¤ì •
                            input_file = os.path.join(
                                output_path,
                                category,
                                "input",
                                rel_path.replace(source_lang_code, LANG_CODE, 1),
                            ).replace("\\", "/")

                            output_file = os.path.join(
                                output_path,
                                category,
                                "output",
                                rel_path.replace(source_lang_code, LANG_CODE, 1),
                            ).replace("\\", "/")

                            # ì´ë¯¸ ë²ˆì—­ëœ íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°
                            if skip_translated and os.path.exists(output_file):
                                await update_progress(worker_id, en_file, 100, True)
                                translated_files[en_file] = output_file
                                return

                            try:
                                # ì…ë ¥ íŒŒì¼ ë‚´ìš© ì¶”ì¶œ
                                en_data = extract_lang_content(en_file)
                                if not en_data:
                                    await update_progress(worker_id, en_file, 100, True)
                                    return

                                # ë°ì´í„°ê°€ ì‚¬ì „ì´ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
                                if not isinstance(en_data, dict):
                                    await update_progress(worker_id, en_file, 100, True)
                                    return

                                # ì…ë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ë° íŒŒì¼ ì €ì¥
                                input_dir = os.path.dirname(input_file)
                                os.makedirs(input_dir, exist_ok=True)

                                with open(input_file, "w", encoding="utf-8") as f:
                                    json.dump(en_data, f, ensure_ascii=False, indent=4)

                                # ë²ˆì—­ ì²˜ë¦¬
                                nonlocal translation_dictionary
                                nonlocal translation_dictionary_lowercase
                                translation_dictionary = sort_and_filter_dictionary()

                                # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
                                temp_output_path = output_file + ".tmp"

                                # ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ë°˜ë“œì‹œ ìƒì„±
                                output_dir = os.path.dirname(output_file)
                                os.makedirs(output_dir, exist_ok=True)

                                # ë²ˆì—­ íŒŒì¼ì˜ ì´ í•­ëª© ìˆ˜
                                total_items = len(en_data)
                                processed_items = 0

                                # ë²ˆì—­ í•¨ìˆ˜ ì •ì˜
                                async def progress_callback():
                                    # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ í•­ëª© ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                                    nonlocal en_data
                                    nonlocal processed_items
                                    items_count = len(en_data)
                                    if items_count > 0:
                                        # ì²˜ë¦¬ëœ í•­ëª© ìˆ˜ ì¦ê°€
                                        processed_items = min(
                                            processed_items + 1,
                                            items_count - 1,
                                        )

                                        # ì§„í–‰ë¥  ê³„ì‚°
                                        progress_percent = int(
                                            (processed_items / items_count) * 100
                                        )

                                        # ì‘ì—…ìì˜ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ (ìµœëŒ€ 95%ê¹Œì§€)
                                        new_progress = min(progress_percent, 95)
                                        await update_progress(
                                            worker_id,
                                            None,
                                            new_progress,
                                            False,
                                            total_items,
                                            processed_items,
                                        )

                                # ë²ˆì—­ ì‹¤í–‰
                                try:
                                    await minecraft_modpack_auto_translator.translate_json_file(
                                        input_path=input_file,
                                        output_path=temp_output_path,  # ì„ì‹œ íŒŒì¼ì— JSONìœ¼ë¡œ ì €ì¥
                                        custom_dictionary_dict=translation_dictionary,
                                        llm=get_translator(
                                            provider=model_provider.lower(),
                                            api_key=current_api_key,
                                            model_name=selected_model,
                                            api_base=api_base_url,
                                            temperature=temperature,
                                            rate_limiter=rate_limiter,
                                        ),
                                        max_workers=file_split_number,  # ë‹¨ì¼ íŒŒì¼ ë‚´ì—ì„œëŠ” ë³‘ë ¬ ì²˜ë¦¬ ì•ˆí•¨
                                        progress_callback=progress_callback,
                                        external_context=shared_context,  # ê³µìœ  ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
                                        delay_manager=delay_manager,
                                        use_random_order=use_random_order,
                                    )

                                    # ëª¨ë“  í•­ëª© ì²˜ë¦¬ ì™„ë£Œ
                                    processed_items = total_items
                                    await update_progress(
                                        worker_id,
                                        None,
                                        95,
                                        False,
                                        total_items,
                                        processed_items,
                                    )

                                    # ì‚¬ì „ ì €ì¥ (ì„ì‹œ íŒŒì¼ì—)
                                    # ë™ì‹œì„± ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ ì‚¬ì „ì˜ ë³µì‚¬ë³¸ ìƒì„±
                                    try:
                                        dictionary_copy = dict(
                                            shared_context.get_dictionary()
                                        )
                                        with open(
                                            os.path.join(
                                                output_path, "shared_dict.json"
                                            ),
                                            "w",
                                            encoding="utf-8",
                                        ) as f:
                                            json.dump(
                                                dictionary_copy,
                                                f,
                                                ensure_ascii=False,
                                                indent=4,
                                            )
                                    except Exception as e:
                                        logger.debug(f"ì‚¬ì „ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

                                    # ì›ë˜ íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                    if os.path.exists(temp_output_path):
                                        file_ext = os.path.splitext(en_file)[1]
                                        parser_class = get_parser_by_extension(file_ext)

                                        if parser_class:
                                            # ì„ì‹œ JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
                                            with open(
                                                temp_output_path, "r", encoding="utf-8"
                                            ) as f:
                                                data = json.load(f)

                                            # ì›ë³¸ íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                            content = parser_class.save(data)

                                            # ìµœì¢… íŒŒì¼ ì €ì¥
                                            with open(
                                                output_file, "w", encoding="utf-8"
                                            ) as f:
                                                f.write(content)

                                            # # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                                            # try:
                                            #     os.remove(temp_output_path)
                                            # except OSError:
                                            #     logger.warning(
                                            #         f"ì„ì‹œ íŒŒì¼ì„ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {temp_output_path}"
                                            #     )

                                            # ì‘ì—… ì™„ë£Œ í‘œì‹œ
                                            await update_progress(
                                                worker_id,
                                                None,
                                                100,
                                                True,
                                                total_items,
                                                total_items,
                                            )
                                        else:
                                            logger.warning(
                                                f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}"
                                            )
                                            await update_progress(
                                                worker_id, None, 100, True
                                            )
                                    else:
                                        logger.warning(
                                            f"ë²ˆì—­ ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {temp_output_path}"
                                        )
                                        await update_progress(
                                            worker_id, None, 100, True
                                        )

                                except Exception as e:
                                    logger.error(
                                        f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì½˜ì†” ì°½ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”."
                                    )
                                    logger.error(traceback.format_exc())

                                    # ì˜¤ë¥˜ íŒŒì¼ ê¸°ë¡
                                    failed_file_info = {
                                        "path": en_file,
                                        "error": str(e),
                                        "category": category,
                                    }
                                    failed_files.append(failed_file_info)

                                    await update_progress(worker_id, en_file, 100, True)

                            except Exception as e:
                                logger.error(
                                    f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì½˜ì†” ì°½ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”."
                                )
                                logger.error(traceback.format_exc())

                                # ì˜¤ë¥˜ íŒŒì¼ ê¸°ë¡
                                failed_file_info = {
                                    "path": en_file,
                                    "error": str(e),
                                    "category": category,
                                }
                                failed_files.append(failed_file_info)

                                await update_progress(worker_id, en_file, 100, True)

                        except Exception as e:
                            error_traceback = traceback.format_exc()
                            logger.error(
                                f"íŒŒì¼ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {str(e)}\n\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì½˜ì†” ì°½ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”."
                            )
                            logger.error(error_traceback)

                            # ì˜¤ë¥˜ íŒŒì¼ ê¸°ë¡
                            failed_file_info = {
                                "path": en_file,
                                "error": str(e),
                                "category": category,
                            }
                            failed_files.append(failed_file_info)

                            await update_progress(worker_id, en_file, 100, True)

                    # ë³‘ë ¬ ë²ˆì—­ ì‹¤í–‰ í•¨ìˆ˜
                    async def run_translation():
                        # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ íŒŒì¼ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
                        all_files = []
                        for category, files in file_types.items():
                            for f in files:
                                all_files.append((f, category))

                        # ì‘ì—… í ìƒì„±
                        queue = asyncio.Queue()

                        # íì— ëª¨ë“  íŒŒì¼ ì¶”ê°€
                        for file_tuple in all_files:
                            await queue.put(file_tuple)

                        # ì›Œì»¤ í•¨ìˆ˜ ì •ì˜
                        async def worker(worker_id):
                            while not queue.empty():
                                try:
                                    file_path, category = await queue.get()
                                    await translate_file(worker_id, file_path, category)
                                    queue.task_done()
                                except Exception as e:
                                    logger.error(f"ì›Œì»¤ {worker_id} ì˜¤ë¥˜: {e}")
                                    queue.task_done()

                        # ì›Œì»¤ ì‹œì‘
                        workers = []
                        for i in range(max_workers):
                            task = asyncio.create_task(worker(i))
                            workers.append(task)

                        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                        await queue.join()

                        # ì›Œì»¤ íƒœìŠ¤í¬ ì·¨ì†Œ
                        for task in workers:
                            task.cancel()

                        # ì·¨ì†Œëœ íƒœìŠ¤í¬ ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°
                        await asyncio.gather(*workers, return_exceptions=True)

                    # ë¹„ë™ê¸° ë²ˆì—­ ì‹¤í–‰
                    asyncio.run(run_translation())

                    # ë¦¬ì†ŒìŠ¤íŒ© ìƒì„± ë° ê²°ê³¼ í†µí•© ZIP ìƒì„±
                    overall_progress_bar.progress(95)
                    overall_progress_text.markdown(
                        f"**{total_files}/{total_files}** (95%)"
                    )
                    status_text.markdown("ë¦¬ì†ŒìŠ¤íŒ© ë° ê²°ê³¼ íŒŒì¼ í†µí•© ì¤‘...")

                    # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ì†ŒìŠ¤íŒ© ì •ë³´
                    categories = {
                        "mods": {
                            "name": "Mods",
                            "suffix": "_MOD_TRANSLATION",
                            "emoji": "ğŸŸ¢",
                            "icon": "ğŸ®",
                            "warning": False,
                        },
                        "config": {
                            "name": "Config",
                            "suffix": "_CONFIG_TRANSLATION",
                            "emoji": "ğŸ”·",
                            "icon": "âš™ï¸",
                            "warning": "Config íŒŒì¼ì€ ëª¨ë“œíŒ©ì— ë”°ë¼ ë¦¬ì†ŒìŠ¤íŒ©ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì••ì¶•ì„ ëª¨ë“œíŒ© Client í´ë”ì— í’€ì–´ì„œ ë®ì–´ì“°ê¸° í•˜ì„¸ìš”.",
                        },
                        "kubejs": {
                            "name": "KubeJS",
                            "suffix": "_KUBEJS_TRANSLATION",
                            "emoji": "ğŸŸ¡",
                            "icon": "ğŸ“œ",
                            "warning": "KubeJS íŒŒì¼ì€ ëª¨ë“œíŒ©ì— ë”°ë¼ ë¦¬ì†ŒìŠ¤íŒ©ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì••ì¶•ì„ ëª¨ë“œíŒ© Client í´ë”ì— í’€ì–´ì„œ ë®ì–´ì“°ê¸° í•˜ì„¸ìš”.",
                        },
                    }

                    # ìƒì„±ëœ ë¦¬ì†ŒìŠ¤íŒ©ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ì™€ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
                    created_resourcepacks = []
                    category_packs = {"All": []}

                    # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ì†ŒìŠ¤íŒ© ìƒì„±
                    for category, info in categories.items():
                        # ì„ íƒë˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬ëŠ” ê±´ë„ˆëœ€
                        if (
                            (category == "config" and not translate_config)
                            or (category == "kubejs" and not translate_kubejs)
                            or (category == "mods" and not translate_mods)
                        ):
                            continue

                        output_dir = os.path.join(output_path, category, "output", "**")
                        output_glob_path = normalize_glob_path(output_dir)
                        if len(glob(output_glob_path, recursive=True)) > 1:
                            # ë¦¬ì†ŒìŠ¤íŒ© ìƒì„±
                            resourcepack_zip = create_resourcepack(
                                output_path,
                                [f"{output_path}/{category}/output"],
                                resourcepack_name + info["suffix"],
                            )

                            # ìƒì„±ëœ ë¦¬ì†ŒìŠ¤íŒ© ì •ë³´ ì €ì¥
                            pack_info = {
                                "category": category,
                                "info": info,
                                "path": resourcepack_zip,
                            }
                            created_resourcepacks.append(pack_info)

                            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ì—ë„ ì¶”ê°€
                            category_name = info["name"]
                            if category_name not in category_packs:
                                category_packs[category_name] = []
                            category_packs[category_name].append(pack_info)
                            category_packs["All"].append(pack_info)

                    # ìµœì¢… ê²°ê³¼ ZIP íŒŒì¼ ìƒì„±
                    final_zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(
                        final_zip_buffer, "w", zipfile.ZIP_DEFLATED
                    ) as final_zip:
                        # ìƒì„±ëœ ë¦¬ì†ŒìŠ¤íŒ© ì¶”ê°€
                        for pack in created_resourcepacks:
                            pack_path = pack["path"]
                            arcname = os.path.basename(
                                pack_path
                            )  # ZIP íŒŒì¼ ë‚´ ê²½ë¡œ (ë£¨íŠ¸ì— ì €ì¥)
                            try:
                                final_zip.write(pack_path, arcname=arcname)
                                logger.info(f"ìµœì¢… ZIPì— ë¦¬ì†ŒìŠ¤íŒ© ì¶”ê°€: {arcname}")
                            except FileNotFoundError:
                                logger.info(
                                    f"ë¦¬ì†ŒìŠ¤íŒ© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pack_path}",
                                    "warning",
                                )
                            except Exception as e:
                                logger.error(
                                    f"ë¦¬ì†ŒìŠ¤íŒ© ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ({pack_path}): {e}"
                                )

                        # ë²ˆì—­ ì‚¬ì „ì„ ZIP íŒŒì¼ì— ì¶”ê°€
                        final_dict_path = os.path.join(
                            output_path, "total_dictionary", f"{uuid_str}_final.json"
                        )
                        os.makedirs(os.path.dirname(final_dict_path), exist_ok=True)
                        with open(final_dict_path, "w", encoding="utf-8") as f:
                            json.dump(
                                translation_dictionary, f, ensure_ascii=False, indent=4
                            )

                        arcname = os.path.join(
                            "dictionary", os.path.basename(final_dict_path)
                        )
                        try:
                            final_zip.write(final_dict_path, arcname=arcname)
                            logger.info(f"ìµœì¢… ZIPì— ì‚¬ì „ íŒŒì¼ ì¶”ê°€: {arcname}")
                        except Exception as e:
                            logger.info(
                                f"ì‚¬ì „ íŒŒì¼ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ({final_dict_path}): {e}",
                                "error",
                            )

                        # ì˜¤ë¥˜ ë°œìƒ íŒŒì¼ ëª©ë¡ ì¶”ê°€
                        failed_list_path = os.path.join(
                            output_path, "failed_files.json"
                        )
                        if os.path.exists(failed_list_path):
                            arcname = "failed_files.json"
                            try:
                                final_zip.write(failed_list_path, arcname=arcname)
                                logger.info(
                                    f"ìµœì¢… ZIPì— ì˜¤ë¥˜ ëª©ë¡ íŒŒì¼ ì¶”ê°€: {arcname}"
                                )
                            except Exception as e:
                                logger.info(
                                    f"ì˜¤ë¥˜ ëª©ë¡ íŒŒì¼ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ({failed_list_path}): {e}",
                                    "error",
                                )

                    # ë¦¬ì†ŒìŠ¤íŒ©ì´ ìƒì„±ë˜ì—ˆì„ ê²½ìš°ì—ë§Œ ê²°ê³¼ í‘œì‹œ
                    if created_resourcepacks:
                        if want_to_share_result:
                            temp_zip_path = os.path.join(temp_dir, "shared_result.zip")
                            with zipfile.ZipFile(
                                temp_zip_path, "w", zipfile.ZIP_DEFLATED
                            ) as temp_zip:
                                # jar_files_fingerprint ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ZIPì— ì¶”ê°€
                                fingerprint_path = os.path.join(
                                    temp_dir, "fingerprint.json"
                                )
                                with open(fingerprint_path, "w", encoding="utf-8") as f:
                                    json.dump(
                                        jar_files_fingerprint,
                                        f,
                                        ensure_ascii=False,
                                        indent=4,
                                    )
                                temp_zip.write(
                                    fingerprint_path, arcname="fingerprint.json"
                                )
                                logger.info("ê³µìœ  ZIPì— ëª¨ë“œ í•‘ê±°í”„ë¦°íŠ¸ ì •ë³´ ì¶”ê°€ ì™„ë£Œ")
                                for (
                                    jar_path,
                                    fingerprint,
                                ) in jar_files_fingerprint.items():
                                    arcname = os.path.basename(jar_path)
                                    extract_path = os.path.join(
                                        output_path,
                                        "mods",
                                        "output",
                                        "extracted",
                                        os.path.basename(jar_path),
                                    ).replace("\\", "/")
                                    try:
                                        path_glob = normalize_glob_path(
                                            os.path.join(extract_path, "**", "*")
                                        )
                                        for src_file in glob(path_glob, recursive=True):
                                            if os.path.isfile(
                                                src_file
                                            ) and not src_file.endswith(".tmp"):
                                                arcname = os.path.join(
                                                    os.path.basename(jar_path),
                                                    os.path.relpath(
                                                        src_file, extract_path
                                                    ),
                                                )
                                                temp_zip.write(
                                                    src_file, arcname=arcname
                                                )
                                    except Exception as e:
                                        logger.error(
                                            f"ZIPì— ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({extract_path}): {e}"
                                        )
                            file_url = catbox_client.upload(temp_zip_path)
                            webhook = DiscordWebhook(
                                url=os.getenv("DISCORD_WEBHOOK_URL"),
                                content=f"CatBox\n{file_url}\n\nëª¨ë¸ ì •ë³´:\n- Provider: {model_provider}\n- Model: {selected_model}\n- Temperature: {temperature}\n- ë³‘ë ¬ ìš”ì²­ ë¶„í• : {file_split_number}\n",
                                thread_name=f"ëª¨ë“œíŒ© ë²ˆì—­ ê²°ê³¼ ({resourcepack_name})",
                            )
                            webhook.execute()
                        st.header("ğŸ¯ ë²ˆì—­ ê²°ê³¼")
                        # íƒ­ ìƒì„± ë° ê²°ê³¼ í‘œì‹œëŠ” ì´ì „ê³¼ ìœ ì‚¬í•˜ê²Œ ìœ ì§€ ê°€ëŠ¥ (ë‹¨, ë‹¤ìš´ë¡œë“œëŠ” í†µí•© ZIPìœ¼ë¡œ)

                        st.warning(
                            "ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ í˜ì´ì§€ê°€ ë¦¬ë¡œë“œ ë©ë‹ˆë‹¤! ì¤‘ìš”í•œ ì •ë³´ê°€ ìœ„ì— ë‚¨ì•„ ìˆë‹¤ë©´ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ëˆ„ë¥´ì§€ ë§ˆì„¸ìš”."
                        )

                        # í†µí•© ZIP ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                        st.download_button(
                            label="ğŸ“¦ ëª¨ë“  ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (.zip)",
                            data=final_zip_buffer.getvalue(),
                            file_name=f"{resourcepack_name}_translation_results.zip",
                            mime="application/zip",
                        )

                    else:
                        st.warning("ë²ˆì—­ëœ íŒŒì¼ì´ ì—†ì–´ ë¦¬ì†ŒìŠ¤íŒ©ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        # ì˜¤ë¥˜ íŒŒì¼ì´ë‚˜ ì‚¬ì „ íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ê²ƒë§Œì´ë¼ë„ ë‹¤ìš´ë¡œë“œ ì œê³µ
                        if os.path.exists(failed_list_path) or os.path.exists(
                            final_dict_path
                        ):
                            st.download_button(
                                label="ğŸ“¦ ê¸°íƒ€ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (.zip)",
                                data=final_zip_buffer.getvalue(),
                                file_name=f"{resourcepack_name}_other_results.zip",
                                mime="application/zip",
                            )

                    # ìµœì¢… ì§„í–‰ ìƒí™©
                    overall_progress_bar.progress(100)
                    overall_progress_text.markdown(
                        f"**{total_files}/{total_files}** (100%)"
                    )
                    status_text.markdown("ë²ˆì—­ ì™„ë£Œ!")

                    # ìµœì¢… ì‚¬ì „ ì €ì¥
                    final_dict_path = os.path.join(
                        output_path, "total_dictionary", f"{uuid_str}_final.json"
                    )
                    os.makedirs(os.path.dirname(final_dict_path), exist_ok=True)
                    # ê³µìœ  ì»¨í…ìŠ¤íŠ¸ì˜ ì‚¬ì „ ì €ì¥
                    shared_dict = shared_context.get_dictionary()
                    with open(final_dict_path, "w", encoding="utf-8") as f:
                        json.dump(shared_dict, f, ensure_ascii=False, indent=4)

                    logger.info(f"ìµœì¢… ê³µìœ  ì‚¬ì „ í¬ê¸°: {len(shared_dict)}ê°œ í•­ëª©")

                    # ê²°ê³¼ í‘œì‹œ
                    st.success(
                        f"ë²ˆì—­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ {len(translated_files)}ê°œì˜ íŒŒì¼ì€ ê±´ë„ˆë›°ê³  ë²ˆì—­ ë˜ì—ˆìŠµë‹ˆë‹¤. ë²ˆì—­ ì‚¬ì „ì€ {len(shared_dict)}ê°œ í•­ëª©ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
                    )

                    # ì˜¤ë¥˜ ë°œìƒ íŒŒì¼ í‘œì‹œ
                    if failed_files:
                        with st.expander(
                            f"ì˜¤ë¥˜ ë°œìƒ íŒŒì¼ ëª©ë¡ ({len(failed_files)}ê°œ)",
                            expanded=False,
                        ):
                            for i, failed_file in enumerate(failed_files):
                                file_basename = os.path.basename(failed_file["path"])
                                st.markdown(
                                    f"**{i + 1}. {file_basename}** ({failed_file['category']})"
                                )
                                st.markdown(f"  - ê²½ë¡œ: `{failed_file['path']}`")
                                st.markdown(f"  - ì˜¤ë¥˜: {failed_file['error']}")
                                st.markdown("---")

                        # ì˜¤ë¥˜ íŒŒì¼ ëª©ë¡ ì €ì¥
                        failed_list_path = os.path.join(
                            output_path, "failed_files.json"
                        )
                        with open(failed_list_path, "w", encoding="utf-8") as f:
                            json.dump(failed_files, f, ensure_ascii=False, indent=4)

                        st.warning(
                            f"ì˜¤ë¥˜ ë°œìƒ íŒŒì¼ ëª©ë¡ì´ {failed_list_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ íŒŒì¼ë“¤ì€ '__FAILED__' ì ‘ë‘ì–´ê°€ ë¶™ì€ íŒŒì¼ë¡œ ë³µì‚¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
                        )

            except Exception:  # ë²ˆì—­ í”„ë¡œì„¸ìŠ¤ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” try ë¸”ë¡ì— ëŒ€í•œ except
                st.error(
                    f"ë²ˆì—­ í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜¤ë¥˜:\n\n{traceback.format_exc()}".replace(
                        "\n", "  \n"
                    )
                )

            # ----- ë²ˆì—­ í”„ë¡œì„¸ìŠ¤ ë -----


if __name__ == "__main__":
    main()
